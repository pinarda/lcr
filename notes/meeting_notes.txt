******************
May 16 Meeting
******************

Write paragraph about bg/zfp algorithm in hayden's technote
Give hayden feedback on Friday
By next week - separate 561 class curriculum into 8 sections
read bit grooming paper - see allison's email (5/25, 10:24am)

    "I just heard this guy (Kouznetsov) talk and he may have already looked
     into the issue of converting bit information by Milan to decimal digits
      with bit grooming (and decided it won't work?). Here is his paper (I
      have not read it yet - I'm still listening to talks)"

DONE - ask lisa if I need to submit hours for the curriculum thing


for documentation, want to have a json or yaml file or something to setup the dataset
want to have a third data type that is specific for less structured data "minimal" maybe


******************
May 9 Meeting
******************

DONE - reply to committe mehmet
DONE - Check for artifacts in Olga's data
DONE - give hayden feedback by wednesday night on his technote
DONE - start abstract in dropbox folder for CESM workshop
DONE  - The hypothesis for figure 18 is that in most regions, there is nothing going on, but in a few small regions the values differ significantly
DONE  - recreate the histograms for alternate thresholds
DONE - Inrepret what is going on in figures 10, 11
RUNNING - use an alternate theshold of 10^-3 for the spatial relative error
LONGER - Investigate whether the spatial relative errors are locally concentrated or more spread out
DONE - Figure out why the Figure citations in the latex doc are messed up.
DONE - Check allison's entropy equation BY TOMORROW AFTERNOON
maybe find a way to show how often the optimal algorithm changes/which metrics are responsible
DONE - stick new plots in report
DONE - update histograms in report
DONE - run models and add to report

******************
April 26 Meeting
******************

DONE - need to move the zfp -1 over to the right
DONE - get results for other metrics, not just zfp
DONE - significant diigits needs to be truncated
DONE - make the test slightly bigger

There's a 538 matplotlib style, can also use seaborn-talk to make text bigger, etc.


DONE - gathering bitgrooming
DONE - create a histogram of how different the hists are using just dssim/all metrics
USE HISTOGRAM - TOO MANY TO LIST? make a decision on how to present the list of cases in the report

CESM workshop on the 13-16th of june, the software one is on June 15th

DJ's group - machine learning group David John Onye(?), Charlie (Tucker?)
Will set a meeting up, try to google how to set this convolutional stuff up.

https://staff.ucar.edu/browse/orgs/AIML
https://staff.ucar.edu/users/dgagne

$25 an hour - 10 hours a week

******************
April 19 Meeting
******************

Hayden Stuff - give him dssim data (anything else?)

formatting the z050
formatting the text size on the histograms
verify the labels on the overall histogram for the ratio
add an arrow - more compression this way

bookmark - we would like to know from the histogram what the distribution is accross each level - think about representing

in bitgrooming we specify the number of significant digits

Comparing Significant Digits derived from real information content and Data SSIM

Talk to dorit about fall payne institue/chris thing
Talk to dorit about methane monitoring stuff - if I can get involved.

******************
April 11 Meeting
******************

DONE - Add NCAR logo to the bottom of the slide/we collaborate with them, they create this nice climate model

The reason it is tapering off is that they don't allow them to store anything else
They put hard limits on individual labs not exceeds ome threshold of storage

Slide 4 - combnine the two sentences, add the "however"clasuse"
bold/italicize "lossy", "lossless" - make it bolder/bigger

instead of saying it doens't work well - mention that we get "limited gains"

at the end of slide 4 -
however, we need to be careful with scientific data"

at the beginning of the four metrics slides - need to tell the story
also that slide - the main metrics we are using is the DSSIM, the others are present to catch specific unusual cases

do the image ssim thing

add a box around the pearson correlation coefficient
talk about it

move text in slide 15 to four metrics slide
remove the mean from the images

"Used as part of an ensemble of metrics, we would ideally like each metric to measure a different type of compression artifact, making it easy to identify what went wrong in the compression."


slide 20 - errors that affect a small spatial area


add color to future work slide - add image where we match compression to datasets
remore the real information slide



mention how the actual data that has been altered
DONE - swap the columns on slide 20
DONE - pearson correlation coefficient varies between -1 and 1
DONE - talk about the 11x11 grid on slide 9 - mention how the boxes are shifted pixel by [ixel
DONE - make the bit on slide 7 not a fragment

******************
April 5 Meeting
******************

add the additional metrics (relative error, ks test, etc, real information?)
see and report on how often it makes a difference (consider other approaches if it doesn't change much)
wavelet scattering - make a presentation ina  few weeks and look into it more
remove the prectmx varaible

see if the varaibles that are conceptually different than the other variables,
because we may need a more complicated modeling approach for them

also-need to work on fine-tuning models - not important right now until we have more data



TO DO MOST URGENT:::::::
speed up metric calculation gathering

In the document - give an overview of the distribution between the zfp and bg levels (include the histograms)
how many times metrics other than the DSSIM mattered


DONE - 32Send Allison an email of what I am running to gather metrics

DONE - FOR ALLISON - GET THE CONSERVATIVE, MIDDLE GROUND, AGGRESSIVE ssims for each time slice for new zfp
DONE = Check with dorit about meeting next week

bg, sz, zfp,

https://carbonplan.org/
******************
March 29 Meeting
******************

Make sure all models return best params
Check out the paper from Allison
put results in report
experiment with leaving out a single climate variable from the model to see if it changes the model.
DONE - figure out how tunstall code handles last letters, find an implementation or look at paper
DONE - point hayden towards some sample data (try p_8-24, and a few more variables. look at the information to see what makes sense)

we don't want to screw up the bit-pattern entopy in the left side of the plot in the limit (as we add more compression)
there should be a way to detect if we have introduced compression artiacats (see the PRECT file) because the entropy on the
left side will be screwed up and not just leveling off as we might hope it would.

We need a way to find where the entropy starts to decay in the compressed data (and levels off in the original data)

******************
March 16 Meeting
******************

People will read the proposal if I present closer to deadline...
but would rather do thesis proposal earlier

Alex todo: Julian Kunkle's code to add to notebook
Fix histograms so they use all the possible levels (combine monthly and daily)
Fix hatching/color on hists

******************
March 10 Meeting
******************

Alex:

want every compression algorithms (and level) displayed on the histograms (even if the frequecny is 0)
use different colors for each compression algorithm (and add shading for each group)
fix the level labeling
reverse the order of the levels

******************
March 1 Meeting
******************

The reason that  sz has a bad compression ratio is that sz does not work well as a preconditioner.
Need to be able to use sz through netCDF the way we are currently using zfp.

Alex:

Look through Julian Kunkle's code to add to notebook
Add the new sz and zfp data to the comparisons
want a histogram that shows the distribtuion of the optimal level for each time slice, grouped by algorithm (do one plot for each variables)
and add a label to the top of the bars)

DONE - Take out the word "alternative" from the new spatial relative error measure, add lines for the 0.05 cutoff and 0.01 cutoff

https://hrsather.medium.com/


******************
Feb 8 Meeting
******************
Right now we divide the difference by the original - want to see if value is 5 or 10% of the original
conservative threshold for multiple points - but have a maximum of 5% difference for a single point

call it alternative spatial error (part 2!)

Allison:

sz data - chunk size 1 will change file size so need to compare across algorithms.

Alex:

read presentations from wednesday
think of practical example of kth order model DONE
read klower paper (allison sent in email) DONE

get entropy of temperature, precipitation data for Hayden's blog DONE

read through 3.2.2 in the book DONE

ldcpy problem allison sent in email (colorbar)

Finish the notebook - last two sections and move the writing sections.
if we want to use speck - make sure it is easy to include another algorithm.
Have all of the ideal algorithms and ratios for all the sample data that we have.


******************
Feb 1 Meeting
******************

Charlie's talk, and David's. There will be slides posted later so I can review.

Peter (zfp) and Frank Capello's (sz) Talk (first two talks), AI and ML talk at the end

Mondays 3-5 xdev meeting "open office hours"

Do problems 3, 4, 6, from the compression textbook

Answers to text snippets: No, Yes, Yes, No, Yes

DONE - Add a small amount to the entire dataset
DONE - Try rounding the data (e.g. converting it to 16 bit)
DONE - Add increasing noise
DONE - Change a single value by a lot (middle of distribution, extremes of distribution)
DONE - Change really small values to zero
DONE - change 5% of the points to fail the spatial relative error (barely)

TS, CLOUD/PRECT



******************
Jan 25 Meeting
******************

The files plots/daily_comp_ratios.png and plots/monthly_comp_ratios.png give the optimal compression ratio for
sz1.4, zfp, and bg. Notice that the sz ratio is always very close to one, which is partially due to the wide spread
of compression levels within each variable for which we choose the least compressed as the optimal level for the
entire variable. Looking at the compression ratios, sz would need an optimal compression level of about 0.01 to be
competitive with the other options which it rarely achieves, so even if there was less spread it would still
underperform the other variables.

The histogram csv files in manual_data/ are easist to look at if opened in excel or saved as a pandas dataframe and
printed to the console as done in scripts/csv_plotter.py. They show the spread of optimal compression levels for each
time slice for each different compression algorithm. There is a large spread of optimal values for sz, a medium spread
for zfp, and a very small spread for bg, whose variables usually fall in only one or two compression bins.

Can we get the filesizes for using zfp not as preconditioner to compare? Are there ways we can improve sz to be
competitive with the other algorithms?


NOTES:

Haiying and Allison can compress netcdf files with zfp, how can you read that data without decompressing it first?
Got it working with netCDF for python, and working on a new kernel in jupyterhub (needed a newer version of netCDF for py)
can use netcdf but not ldcpy on new kernel.

Should be aware of how we are compressing the data (our data is compressed "all at once",
but when we are performing prediction we will have to make predicitons for a single
time slice. In general - make sure we are testing the full range of possibilities (want bounds that
are not used to show we have the right levels dialed in)

Todos:
Alex:

Read 2.1, 2.2 of compression book. Have ideas about how Hayden can contribute.

DONE --- Find the time slice that corresponds to each of the optimal levels (for every variable in the table)

Think about writing a paper/thesis - note that we are not interested in the sz tolerances 0.1 to 0.005
because we tested them and found that they were not optimal for any of the compression levels.

DONE --- add texture to the plots (to make it easier for colorblind people (stripes, dots, etc))

interested in looking at the DSSIM that is optimal for each algorithm - do the algorithms that perform
better have DSSIMS right near 0.9995 while the ones that perform worse (in terms of ratio) have higher
DSSIMS?

redo plots/tables for sz when ready
Also redo for everything when Allison runs them using a chunk size of 1 since it will slightly change the compression
Fix up the dssim gathering script to speedup gathering:
    run dssim on all 730 time steps for the daily variables
    finish computing dssim for the 60 3D monthly variables


******************
Jan 11 Meeting
******************

Bitgrooming is a contender for compression - Charlie Zender

We were applying zip as 1d data - this is not ideal in terms of compression ratio.
Would be better to compress as 2d or 3d data.
Using zfp as a preconditioner - so the things that happen to the trailing bits can be random, so it is less good as a preconditioned than just cutting them all to zero

Need to be able to use netcdf support for zfp if we don’t use it as a preconditioner

Need to have a version of netcdf that uses hdf5 - version on Cheyenne does not have it currently

No hdf5 filter for sz - have to use as preconditioner

Bit grooming, try 2-7 for bit grooming on each variable.


We can get better compression using the absolute error mode potentially, but it makes less sense to Allison because absolute error is not as general across variables.


Alex:

Describe the data

Look at the LENS2 data, how it is all organized,

Look at the web page. Confused until studied the figure that they made at the bottom

Spend time reading the CESM-LENS2/COSP documentation

Original data can be found at the beginning of the documentation, see https://www.cesm.ucar.edu/projects/community-projects/LENS2/data-sets.html




Start going in to NCAR February on Tuesday afternoons


Allison sent error analysis zfp paper to email - will discuss later in semester after working through some of the textbook