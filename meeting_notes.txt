******************
Feb 1 Meeting
******************

Charlie's talk, and David's. There will be slides posted later so I can review.

Peter (zfp) and Frank Capello's (sz) Talk (first two talks), AI and ML talk at the end

Mondays 3-5 xdev meeting "open office hours"

Do problems 3, 4, 6, from the compression textbook

Answers to text snippets: No, Yes, Yes, No, Yes

DONE - Add a small amount to the entire dataset
DONE - Try rounding the data (e.g. converting it to 16 bit)
DONE - Add increasing noise
DONE - Change a single value by a lot (middle of distribution, extremes of distribution)
DONE - Change really small values to zero
DONE - change 5% of the points to fail the spatial relative error (barely)

TS, CLOUD/PRECT



******************
Jan 25 Meeting
******************

The files plots/daily_comp_ratios.png and plots/monthly_comp_ratios.png give the optimal compression ratio for
sz1.4, zfp, and bg. Notice that the sz ratio is always very close to one, which is partially due to the wide spread
of compression levels within each variable for which we choose the least compressed as the optimal level for the
entire variable. Looking at the compression ratios, sz would need an optimal compression level of about 0.01 to be
competitive with the other options which it rarely achieves, so even if there was less spread it would still
underperform the other variables.

The histogram csv files in manual_data/ are easist to look at if opened in excel or saved as a pandas dataframe and
printed to the console as done in scripts/csv_plotter.py. They show the spread of optimal compression levels for each
time slice for each different compression algorithm. There is a large spread of optimal values for sz, a medium spread
for zfp, and a very small spread for bg, whose variables usually fall in only one or two compression bins.

Can we get the filesizes for using zfp not as preconditioner to compare? Are there ways we can improve sz to be
competitive with the other algorithms?


NOTES:

Haiying and Allison can compress netcdf files with zfp, how can you read that data without decompressing it first?
Got it working with netCDF for python, and working on a new kernel in jupyterhub (needed a newer version of netCDF for py)
can use netcdf but not ldcpy on new kernel.

Should be aware of how we are compressing the data (our data is compressed "all at once",
but when we are performing prediction we will have to make predicitons for a single
time slice. In general - make sure we are testing the full range of possibilities (want bounds that
are not used to show we have the right levels dialed in)

Todos:
Alex:

Read 2.1, 2.2 of compression book. Have ideas about how Hayden can contribute.

DONE --- Find the time slice that corresponds to each of the optimal levels (for every variable in the table)

Think about writing a paper/thesis - note that we are not interested in the sz tolerances 0.1 to 0.005
because we tested them and found that they were not optimal for any of the compression levels.

DONE --- add texture to the plots (to make it easier for colorblind people (stripes, dots, etc))

interested in looking at the DSSIM that is optimal for each algorithm - do the algorithms that perform
better have DSSIMS right near 0.9995 while the ones that perform worse (in terms of ratio) have higher
DSSIMS?

redo plots/tables for sz when ready
Also redo for everything when Allison runs them using a chunk size of 1 since it will slightly change the compression
Fix up the dssim gathering script to speedup gathering:
    run dssim on all 730 time steps for the daily variables
    finish computing dssim for the 60 3D monthly variables


******************
Jan 11 Meeting
******************

Bitgrooming is a contender for compression - Charlie Zender

We were applying zip as 1d data - this is not ideal in terms of compression ratio.
Would be better to compress as 2d or 3d data.
Using zfp as a preconditioner - so the things that happen to the trailing bits can be random, so it is less good as a preconditioned than just cutting them all to zero

Need to be able to use netcdf support for zfp if we donâ€™t use it as a preconditioner

Need to have a version of netcdf that uses hdf5 - version on Cheyenne does not have it currently

No hdf5 filter for sz - have to use as preconditioner

Bit grooming, try 2-7 for bit grooming on each variable.


We can get better compression using the absolute error mode potentially, but it makes less sense to Allison because absolute error is not as general across variables.


Alex:

Describe the data

Look at the LENS2 data, how it is all organized,

Look at the web page. Confused until studied the figure that they made at the bottom

Spend time reading the CESM-LENS2/COSP documentation

Original data can be found at the beginning of the documentation, see https://www.cesm.ucar.edu/projects/community-projects/LENS2/data-sets.html




Start going in to NCAR February on Tuesday afternoons


Allison sent error analysis zfp paper to email - will discuss later in semester after working through some of the textbook